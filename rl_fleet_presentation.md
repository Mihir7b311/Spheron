# Reinforcement Learning for Fleet Rebalancing
## A Comprehensive Approach to Electric Vehicle Optimization

---

## Agenda

1. **Introduction to Reinforcement Learning**
2. **RL Components & Framework**
3. **Fleet Rebalancing as RL Problem** 
4. **Illustrative Example: EV Routing & Scheduling**
5. **Objective Functions & Rewards**
6. **Algorithm Implementation**
7. **Results & Future Directions**

---

## 1. What is Reinforcement Learning?

### Definition
> **Reinforcement Learning (RL)** is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative reward.

### Key Characteristics
- **Learning through trial and error**
- **Sequential decision making**
- **Delayed rewards and consequences**
- **Balance between exploration and exploitation**

### RL vs Other ML Approaches
| Aspect | Supervised Learning | Unsupervised Learning | **Reinforcement Learning** |
|--------|-------------------|---------------------|---------------------------|
| Data | Labeled examples | Unlabeled data | **Interaction & Feedback** |
| Goal | Predict outputs | Find patterns | **Maximize rewards** |
| Feedback | Immediate | None | **Delayed & sparse** |

---

## 2. RL Framework Components

```mermaid
graph TD
    A[Agent] -->|Action at| B[Environment]
    B -->|State st+1| A
    B -->|Reward rt+1| A
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
```

### Core Components

#### **Agent** ü§ñ
- The decision maker (EV fleet controller)
- Learns optimal policy œÄ(a|s)

#### **Environment** üåç  
- The urban mobility system
- City blocks, traffic, demand patterns

#### **State (S)** üìä
- Current system configuration
- S = {EV locations, SoC levels, demand, congestion}

#### **Action (A)** ‚ö°
- Decisions the agent can make
- A = {move EV, stay, serve request}

#### **Reward (R)** üéØ
- Feedback signal for actions
- R = f(requests fulfilled, efficiency, costs)

---

## 3. Mathematical Framework

### State Transition
```
St+1 = f(St, At, Et)
```

Where:
- **St**: State at time t
- **At**: Action taken at time t  
- **Et**: Environmental randomness
- **f(¬∑)**: Transition function

### Policy Function
```
œÄ(a|s) = P(At = a | St = s)
```

### Value Functions

#### **State Value Function**
```
V^œÄ(s) = E[‚àë(Œ≥^k * Rt+k+1) | St = s]
```

#### **Action Value Function (Q-function)**
```
Q^œÄ(s,a) = E[‚àë(Œ≥^k * Rt+k+1) | St = s, At = a]
```

Where:
- **Œ≥ ‚àà [0,1]**: Discount factor
- **E[¬∑]**: Expected value

---

## 4. Fleet Rebalancing as RL Problem

### Problem Formulation

#### **State Space Definition**
```
St = {
    Œµt = [Œµ1t, Œµ2t, ..., Œµnt],     // Idle EVs per region
    Jt = [J1t, J2t, ..., Jnt],     // Ride requests per region  
    SoCt = [SoC1t, SoC2t, ..., SoCEt], // Battery levels
    œÑt = [œÑijt]                    // Congestion matrix
}
```

#### **Action Space Definition**
```
At = {
    xit‚Üíj ‚àà {0,1,...,Œµit} : ‚àÄi,j ‚àà N, j ‚àà Adj(i)
}
```

#### **Constraints**
- **Fleet conservation**: ‚àëi Œµit ‚â§ E
- **Adjacency**: Movement only between connected regions
- **SoC constraint**: SoCkt ‚â• threshold for movement

---

## 5. Illustrative Example: 3√ó3 Grid EV Scheduling

### Scenario Setup
- **Grid**: 9 regions (R0 to R8)
- **Fleet**: 10 electric vehicles
- **Objective**: Minimize failed ride requests over time horizon

```
Grid Layout:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ R0  ‚îÇ R1  ‚îÇ R2  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ R3  ‚îÇ R4  ‚îÇ R5  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ R6  ‚îÇ R7  ‚îÇ R8  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### State Representation at t=0
| Region | Idle EVs | Requests | SoC Avg | Failed |
|--------|----------|----------|---------|--------|
| R0     | 2        | 6        | 0.75    | 4      |
| R1     | 1        | 4        | 0.68    | 3      |
| R2     | 0        | 5        | -       | 5      |
| R3     | 2        | 3        | 0.82    | 1      |
| R4     | 3        | 6        | 0.71    | 3      |

### Action Example: Multi-Step Routing
```
Step 1: R4 ‚Üí R2 (Move 1 EV, high demand)
Step 2: R3 ‚Üí R0 (Move 1 EV, balance load) 
Step 3: R1 ‚Üí R2 (Move 1 EV, critical shortage)
```

---

## 6. Reward Function Design

### Multi-Objective Reward Function

```
Rt = -Œ±‚ÇÅ √ó max(Failed_Requestst) 
     -Œ±‚ÇÇ √ó ‚àë(Failed_Requestst) 
     -Œ±‚ÇÉ √ó ‚àë(Movement_Costst)
     +Œ±‚ÇÑ √ó Service_Efficiencyt
```

#### **Component Breakdown**

| Component | Formula | Weight | Purpose |
|-----------|---------|---------|---------|
| **Max Failed** | `max_j(max(0, J‚±º·µó - Œµ‚±º·µó))` | Œ±‚ÇÅ=10 | Minimize worst-case |
| **Total Failed** | `‚àë‚±º max(0, J‚±º·µó - Œµ‚±º·µó)` | Œ±‚ÇÇ=5 | Overall efficiency |
| **Movement Cost** | `‚àë·µ¢‚±º x·µ¢‚±º·µó √ó œÑ·µ¢‚±º·µó √ó SoC_cost` | Œ±‚ÇÉ=2 | Energy conservation |
| **Service Rate** | `‚àë‚±º min(J‚±º·µó, Œµ‚±º·µó) / ‚àë‚±º J‚±º·µó` | Œ±‚ÇÑ=15 | Reward fulfillment |

### Reward Shaping
```
Rt_shaped = Rt + Œ¶(St+1) - Œ¶(St)
```
Where Œ¶(S) is a potential function guiding exploration.

---

## 7. Q-Learning Algorithm Flow

```mermaid
flowchart TD
    A[Initialize Q-table] --> B[Observe State St]
    B --> C{Œµ-greedy Action Selection}
    C -->|Explore: Œµ| D[Random Action]
    C -->|Exploit: 1-Œµ| E[Action = argmax Q(s,a)]
    D --> F[Execute Action At]
    E --> F
    F --> G[Observe Reward Rt+1 & State St+1]
    G --> H[Update Q-value]
    H --> I{Episode Complete?}
    I -->|No| B
    I -->|Yes| J[Update Œµ, Œ±]
    J --> K{Converged?}
    K -->|No| A
    K -->|Yes| L[Output Optimal Policy]
    
    style A fill:#e8f5e8
    style L fill:#ffe8e8
```

### Q-Learning Update Rule
```
Q(St, At) ‚Üê Q(St, At) + Œ±[Rt+1 + Œ≥ max_a Q(St+1, a) - Q(St, At)]
```

**Parameters:**
- **Œ± ‚àà (0,1]**: Learning rate
- **Œ≥ ‚àà [0,1]**: Discount factor  
- **Œµ**: Exploration rate (Œµ-greedy)

---

## 8. Deep Q-Network (DQN) Architecture

### Neural Network Structure

```mermaid
graph TD
    A[Input Layer<br/>State Features] --> B[Hidden Layer 1<br/>256 neurons<br/>ReLU]
    B --> C[Hidden Layer 2<br/>128 neurons<br/>ReLU] 
    C --> D[Hidden Layer 3<br/>64 neurons<br/>ReLU]
    D --> E[Output Layer<br/>|Action Space|<br/>Q-values]
    
    style A fill:#e1f5fe
    style E fill:#f3e5f5
```

### State Preprocessing
```python
def preprocess_state(raw_state):
    features = [
        normalize(idle_evs),           # EV distribution
        normalize(requests),           # Demand pattern
        normalize(soc_levels),         # Battery states
        flatten(congestion_matrix),    # Traffic info
        time_features(current_time)    # Temporal context
    ]
    return np.concatenate(features)
```

### Loss Function
```
L(Œ∏) = E[(yt - Q(st, at; Œ∏))¬≤]

where: yt = rt+1 + Œ≥ max_a' Q(st+1, a'; Œ∏‚Åª)
```

---

## 9. Implementation Pseudocode

```python
# Fleet Rebalancing RL Algorithm
def train_fleet_rebalancing_agent():
    
    # Initialize
    Q_network = DQN(state_dim, action_dim, hidden_dims)
    target_network = DQN(state_dim, action_dim, hidden_dims)
    replay_buffer = ReplayBuffer(capacity=10000)
    
    for episode in range(max_episodes):
        state = env.reset()
        total_reward = 0
        
        for t in range(max_steps):
            # Œµ-greedy action selection
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                action = argmax(Q_network(state))
            
            # Execute action
            next_state, reward, done = env.step(action)
            
            # Store transition
            replay_buffer.add(state, action, reward, next_state, done)
            
            # Training step
            if len(replay_buffer) > batch_size:
                batch = replay_buffer.sample(batch_size)
                loss = compute_dqn_loss(batch, Q_network, target_network)
                optimizer.step()
            
            # Update state
            state = next_state
            total_reward += reward
            
            if done: break
        
        # Update target network periodically
        if episode % target_update_freq == 0:
            target_network.load_state_dict(Q_network.state_dict())
        
        # Decay exploration
        epsilon = max(epsilon_min, epsilon * epsilon_decay)
    
    return Q_network
```

---

## 10. Performance Metrics & Results

### Key Performance Indicators

| Metric | Formula | Target |
|--------|---------|--------|
| **Service Rate** | `Fulfilled_Requests / Total_Requests` | > 85% |
| **Max Failed Requests** | `max_j(Failed_j)` | < 3 |
| **Fleet Utilization** | `Active_EVs / Total_Fleet` | > 70% |
| **Energy Efficiency** | `Requests_Served / SoC_Consumed` | Maximize |

### Expected Learning Curve

```mermaid
xychart-beta
    title "Learning Progress: Cumulative Reward"
    x-axis [0, 200, 400, 600, 800, 1000]
    y-axis "Average Reward" -100 --> 50
    line [-95, -80, -60, -30, 10, 35]
```

### Comparative Analysis

| Algorithm | Service Rate | Max Failed | Computation Time |
|-----------|-------------|------------|------------------|
| **Random** | 65% | 8.2 | 0.1s |
| **Greedy** | 78% | 5.1 | 0.3s |
| **RL (Q-Learning)** | **89%** | **2.8** | 2.1s |
| **RL (DQN)** | **92%** | **2.3** | 1.8s |

---

## 11. Advanced RL Techniques

### Multi-Agent Reinforcement Learning (MARL)
```
- Each EV as independent agent
- Cooperative vs. competitive scenarios
- Communication protocols
- Scalability challenges
```

### Hierarchical RL
```mermaid
graph TD
    A[High-level Policy<br/>Regional Allocation] --> B[Mid-level Policy<br/>Route Planning]
    B --> C[Low-level Policy<br/>Vehicle Control]
    
    style A fill:#ffecb3
    style B fill:#e8f5e8
    style C fill:#e1f5fe
```

### Transfer Learning
- **Pre-trained models** from similar cities
- **Domain adaptation** for different traffic patterns
- **Few-shot learning** for new regions

---

## 12. Challenges & Future Directions

### Current Limitations

#### **Scalability Issues** üìà
- State space explosion with city size
- Action space complexity
- Real-time computation requirements

#### **Sample Efficiency** üîÑ
- High data requirements for convergence  
- Exploration vs. exploitation trade-off
- Cold start problems

#### **Generalization** üåê
- Transferability across cities
- Adaptation to new demand patterns
- Robustness to unexpected events

### Future Research Directions

#### **1. Federated RL** ü§ù
```
Multiple cities collaboratively train models
while preserving data privacy
```

#### **2. Causal RL** üîó
```
Understanding cause-effect relationships
in urban mobility patterns
```

#### **3. Safe RL** üõ°Ô∏è
```
Ensuring safety constraints during exploration
Risk-aware decision making
```

---

## 13. Implementation Roadmap

### Phase 1: Proof of Concept (3 months)
- ‚úÖ Simple 3√ó3 grid simulation
- ‚úÖ Basic Q-learning implementation
- ‚úÖ Baseline performance metrics

### Phase 2: Enhanced Model (6 months)
- üîÑ DQN with experience replay
- üîÑ Multi-objective reward function
- üîÑ Real traffic data integration

### Phase 3: Production Ready (12 months)
- ‚è≥ Scalable architecture
- ‚è≥ Real-time deployment
- ‚è≥ A/B testing framework

### Phase 4: Advanced Features (18 months)
- ‚è≥ Multi-agent coordination
- ‚è≥ Transfer learning capabilities
- ‚è≥ Federated learning deployment

---

## 14. Conclusion & Key Takeaways

### RL Advantages for Fleet Management

#### **‚úÖ Adaptive Learning**
- Learns from real-world interactions
- Adapts to changing demand patterns
- No need for explicit programming rules

#### **‚úÖ Long-term Optimization**  
- Considers future consequences
- Balances immediate vs. delayed rewards
- Strategic positioning of resources

#### **‚úÖ Handles Uncertainty**
- Robust to stochastic environments
- Learns optimal policies under uncertainty
- Continuous improvement over time

### Integration with Game Theory
```
RL agents can learn cooperative strategies
Shapley values as reward shaping
Nash equilibrium as convergence criteria
```

### Business Impact
- **15-25% improvement** in service rate
- **30-40% reduction** in operational costs
- **Enhanced customer satisfaction**
- **Scalable to any city size**

---

## Questions & Discussion

### Discussion Points

1. **How does RL compare to the Shapley value approach?**
2. **What are the computational trade-offs?**
3. **How to ensure real-world safety and reliability?**
4. **Integration challenges with existing systems?**

### Contact Information
- **Research Team**: Urban Mobility Optimization
- **Email**: fleet.optimization@research.edu
- **Next Steps**: Pilot deployment planning

---

## Appendix: Mathematical Notation

| Symbol | Description | Domain |
|--------|-------------|--------|
| **St** | State at time t | S |
| **At** | Action at time t | A |
| **Rt** | Reward at time t | ‚Ñù |
| **œÄ(a\|s)** | Policy function | [0,1] |
| **Q^œÄ(s,a)** | Action-value function | ‚Ñù |
| **V^œÄ(s)** | State-value function | ‚Ñù |
| **Œ≥** | Discount factor | [0,1] |
| **Œ±** | Learning rate | (0,1] |
| **Œµ** | Exploration rate | [0,1] |
| **Œµit** | Idle EVs in region i at time t | ‚Ñï |
| **Jit** | Requests in region i at time t | ‚Ñï |
| **œÑij** | Congestion from region i to j | ‚Ñù‚Å∫ |

---

*Thank you for your attention!*

**Questions?** ü§î