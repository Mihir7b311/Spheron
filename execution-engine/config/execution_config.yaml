# config/execution_config.yaml

runtime:
  max_concurrent: 10
  timeout_seconds: 300
  enable_profiling: true
  profiling_interval: 1.0

batch:
  max_batch_size: 32
  min_batch_size: 1
  wait_timeout: 0.1
  max_queue_size: 1000
  dynamic_batching: true
  optimization_interval: 5.0

cuda:
  default_memory_fraction: 0.8
  enable_tensorflow: true
  enable_pytorch: true
  max_streams_per_gpu: 16
  memory_allocation:
    initial_pool_size: "2GB"
    max_pool_size: "8GB"
    allocation_step: "512MB"
  
streams:
  default_priority: 0
  max_active_streams: 8
  sync_timeout: 1.0

monitoring:
  enable_metrics: true
  collection_interval: 1.0
  history_size: 1000
  alert_thresholds:
    memory_usage: 0.9
    compute_usage: 0.9
    execution_time: 1000
    batch_latency: 100